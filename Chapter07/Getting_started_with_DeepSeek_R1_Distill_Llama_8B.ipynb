{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNGKyUIM6sT9Fgu4rdCIiZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "751185796314473f9087cacb971c0933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1294c2b9211f4c8a8871ed0228cb82dd",
              "IPY_MODEL_691ff290898249b28dd9dd251b0c3691",
              "IPY_MODEL_c43241ee018d426aa8eb5177676664c8"
            ],
            "layout": "IPY_MODEL_9e160081fa00479c9b4a49fd706f2d71"
          }
        },
        "1294c2b9211f4c8a8871ed0228cb82dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8698950f901495189757a0285ca4f6a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_47d4f1c1740941959a30b2d7f38a50f5",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "691ff290898249b28dd9dd251b0c3691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6a4d6f8d854c75baafe041d2da72da",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f779a1a590414dd2afb28474449af582",
            "value": 4
          }
        },
        "c43241ee018d426aa8eb5177676664c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb0e7ca7ce84e73b3dd9cdb0f6c8a06",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9f1bd40c562040a8b9fe1d4d1905a794",
            "value": "‚Äá4/4‚Äá[00:13&lt;00:00,‚Äá‚Äá3.04s/it]"
          }
        },
        "9e160081fa00479c9b4a49fd706f2d71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8698950f901495189757a0285ca4f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d4f1c1740941959a30b2d7f38a50f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f6a4d6f8d854c75baafe041d2da72da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f779a1a590414dd2afb28474449af582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4eb0e7ca7ce84e73b3dd9cdb0f6c8a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1bd40c562040a8b9fe1d4d1905a794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Getting Started with DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìå **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "---\n",
        "\n",
        "## **üöÄ Installing and running DeepSeek-R1-Distill-Llama-8B**  \n",
        "\n",
        "This notebook provides a **step-by-step guide** on how to **download and run DeepSeek-R1-Distill-Llama-8B** locally in **Google Drive**.  The version downloaded is an open-source distilled version of DeepSeek-R1 provided by  unsloth, an LLM accelerator,  on Hugging Face :https://unsloth.ai/\n",
        "\n",
        "If you don't want to use Google Drive, you can install the artefacts on a local machine, server or cloud server.\n",
        "\n",
        "### **üîπ How to Get Started**  \n",
        "1Ô∏è‚É£ **Install the model's artifacts** ‚Üí Set `install_deepseek=True` and run all cells.  \n",
        "2Ô∏è‚É£ **Restart the session** ‚Üí Disconnect and start a new session.  \n",
        "3Ô∏è‚É£ **Re-run the model** ‚Üí Set `install_deepseek=False` and run all cells again.  \n",
        "4Ô∏è‚É£ **Interact with the model** ‚Üí Use it in a prompt session!  \n",
        "\n",
        "‚ö†Ô∏è **System Requirements**  \n",
        "‚úÖ **GPU** ‚Äì Minimum **16GB** VRAM required.  \n",
        "‚úÖ **Google Drive Space** ‚Äì At least **20GB** free space.  \n",
        "üìå **Educational Use Only** ‚Äì For production, deploy artifacts on a **local or cloud server**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìñ Table of Contents**  \n",
        "\n",
        "### **1Ô∏è‚É£ Setting Up the DeepSeek Environment (Hugging Face)**  \n",
        "‚úÖ Checking GPU Activation  \n",
        "üìÇ Mounting Google Drive  \n",
        "‚öôÔ∏è Installing the Hugging Face Environment  \n",
        "üîÑ Ensuring `install_deepseek=True` for First Run  \n",
        "üìå Checking Transformer Version  \n",
        "\n",
        "### **2Ô∏è‚É£ Downloading DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìÇ Verifying Download Path  \n",
        "\n",
        "### **3Ô∏è‚É£ Running a DeepSeek Session**  \n",
        "üîÑ Setting `install_deepseek=False` for Second Run  \n",
        "üìå Model Information  \n",
        "üí¨ Running an Interactive Prompt Session  \n",
        "\n",
        "---\n",
        "\n",
        "### **üí° Ready to Use DeepSeek?**  \n",
        "Follow the **installation steps**, ensure you have the required **hardware**, and launch your **interactive AI session** üöÄ"
      ],
      "metadata": {
        "id": "dB7uI-BJ94pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting up DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set install_deepseek to True to download and install R1 Distill Llama 8B locally\n",
        "# Set install_deepseek to False to run an R1 session\n",
        "install_deepseek=False"
      ],
      "metadata": {
        "id": "81qmq4cJ65_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lnpx8Ywvkqu",
        "outputId": "b9e81eba-7cdf-403b-da3b-b12b3de87ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  5 08:11:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJAvtheKuudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4b223d-109a-459c-f0b5-b8585351275a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the cache directory in your Google Drive\n",
        "cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "# Set environment variables to direct Hugging Face to use this cache directory\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "#os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip transformers==4.48.3"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a793bd-3422-4456-e4a4-35837cb087c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"transformers==4.48.3\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.DeepSeek download\n",
        "\n"
      ],
      "metadata": {
        "id": "CMDClDdHJ1nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "if install_deepseek==True:\n",
        "   # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  model_name = 'unsloth/DeepSeek-R1-Distill-Llama-8B'\n",
        "  # Load the tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
        "\n",
        "    # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "FT1zO4ShzzON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b49839b-3482-49e3-abeb-6667e4652856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        " !ls -R /content/drive/MyDrive/genaisys/HuggingFaceCache"
      ],
      "metadata": {
        "id": "zz0hLeEhJTt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472429b8-467f-459b-d928-37e5273de51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/genaisys/HuggingFaceCache:\n",
            "models--unsloth--DeepSeek-R1-Distill-Llama-8B  version.txt\n",
            "\n",
            "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B:\n",
            "blobs  refs  snapshots\n",
            "\n",
            "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/blobs:\n",
            "03910325923893259d090bfa92baa4088cd46573\n",
            "0ab389d23c02726e56c53379f99a420035974a33\n",
            "0c15378b8bf8af3ceaa5e7a81372996b5080fe2035fd304b491064f95b8625e2\n",
            "0fd8120f1c6acddc268ebc2583058efaf699a771\n",
            "13263c27b6e1c82a791559fc2fe27af0748060180c559220d45b93b5fffe239e\n",
            "21b8ca8f9ab09417c124d32ba5b9a59bcd417c4594e41a48d3e869a8a328a021\n",
            "49d6c171706a9c36a4ba5f358e9ce94c27557fa812da99aa5ef0961fcd35de3f\n",
            "846e0e5df0c5f053a21f9390ceec3eabc52d06b3\n",
            "afcf8b83f782748e77f548ee46a21ced225f3431\n",
            "d91915040cfac999d8c55f4b5bc6e67367c065e3a7a4e4b9438ce1f256addd86\n",
            "\n",
            "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/refs:\n",
            "main\n",
            "\n",
            "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots:\n",
            "71f34f954141d22ccdad72a2e3927dddf702c9de\n",
            "\n",
            "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de:\n",
            "config.json\t\t\t  model-00003-of-00004.safetensors  tokenizer_config.json\n",
            "generation_config.json\t\t  model-00004-of-00004.safetensors  tokenizer.json\n",
            "model-00001-of-00004.safetensors  model.safetensors.index.json\n",
            "model-00002-of-00004.safetensors  special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.DeepSeek-R1-Distill-Llama-8B session"
      ],
      "metadata": {
        "id": "mPFjy90U2gcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "if install_deepseek==False:\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "751185796314473f9087cacb971c0933",
            "1294c2b9211f4c8a8871ed0228cb82dd",
            "691ff290898249b28dd9dd251b0c3691",
            "c43241ee018d426aa8eb5177676664c8",
            "9e160081fa00479c9b4a49fd706f2d71",
            "b8698950f901495189757a0285ca4f6a",
            "47d4f1c1740941959a30b2d7f38a50f5",
            "5f6a4d6f8d854c75baafe041d2da72da",
            "f779a1a590414dd2afb28474449af582",
            "4eb0e7ca7ce84e73b3dd9cdb0f6c8a06",
            "9f1bd40c562040a8b9fe1d4d1905a794"
          ]
        },
        "outputId": "17ba1cce-893e-42fa-bbff-fa48683df56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "751185796314473f9087cacb971c0933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 14.71 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  print(model.config)"
      ],
      "metadata": {
        "id": "jSgiBU2m8rJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a92382-85fc-4863-9e2d-0fdfc676204b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "QTSNYBCb4vtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  prompt=\"\"\"\n",
        "  Explain how a product designer could transform customer requirements for a traveling bag into a production plan.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "t99pgv0IQ30m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if install_deepseek==False:\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  # Tokenize the input\n",
        "  inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Generate output with enhanced anti-repetition settings\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1200,\n",
        "    repetition_penalty=1.5,             # Increase penalty to 1.5 or higher\n",
        "    no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "    temperature=0.6,                    # Reduce randomness slightly\n",
        "    top_p=0.9,                          # Nucleus sampling for diversity\n",
        "    top_k=50                            # Limits token selection to top-k probable tokens\n",
        "  )\n",
        "\n",
        "  # Decode and display the output\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Qt9GTrB34r2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cb952f-259c-48ff-924b-ded78f3111c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 20.61 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "if install_deepseek==False:\n",
        "  wrapped_text = textwrap.fill(generated_text, width=80)\n",
        "  print(wrapped_text)"
      ],
      "metadata": {
        "id": "p4QX8niIK6Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d770bb-32f6-49f4-8189-dc9dc162c14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Explain how a product designer could transform customer requirements for a\n",
            "traveling bag into a production plan.       To explain the process of\n",
            "transforming customer requirements into a prototype and then to mass production,\n",
            "I will break it down step by step.  Firstly, understanding what customers need\n",
            "is crucial. The user has specified that they want an improved version of my\n",
            "existing Travel Bag Design Concept (TBD). They have provided some initial\n",
            "feedback on their preferences regarding materials, functionality, comfort\n",
            "features like back support or strap adjustments.   Next comes defining clear\n",
            "objectives based on this information: What exactly are we aiming to improve? Is\n",
            "there something specific about ergonomics, durability, ease-of-use?  Then\n",
            "gathering all necessary details from users through surveys, interviews etc.,\n",
            "which might include aspects such as desired weight capacity in different models;\n",
            "preferred types of zippers or buckles used;  After collecting data\n",
            "systematically, organizing these inputs can help prioritize needs‚Äîperhaps\n",
            "creating categories like \"Must-Have,\" \"Nice-to-have\" versus ‚ÄúNon-essential‚Äù\n",
            "attributes helps manage expectations against potential trade-offs later on\n",
            "during design decisions;    Once goals & priorities become clearer, developing\n",
            "prototypes becomes more focused since each iteration would aim at testing one\n",
            "main feature rather than multiple changes simultaneously‚Äîwhich makes refining\n",
            "individual elements easier before moving towards finalizing designs,     When\n",
            "prototyping starts: 1) Start with basic functional mockups using simple tools ‚Äì\n",
            "paper sketches maybe even quick digital renders if possible‚Äîto visualize\n",
            "concepts quickly without heavy investment yet still capture key proportions and\n",
            "relationships between parts,  2) Rapid Prototying Machines: Utilize CNC machines\n",
            "or laser cutters not just limited to making physical objects but also\n",
            "experimenting creatively within constraints‚Äîfor instance trying out various\n",
            "zipper placements or exploring alternative fastening mechanisms visually\n",
            "beforehand saves time when transitioning to actual manufacturing processes  3)\n",
            "Functionality Testing Phase: Test every component under real-world conditions\n",
            "simulated environments where you check things like opening/closing speed over\n",
            "extended use periods whether straps hold up after repeated stress tests...  4)\n",
            "Iterate Based On Feedback:  If any part fails during function checks get notes\n",
            "on why‚Äîit‚Äôs essential noting both positive reinforcement (\"This aspect works\n",
            "well\") alongside areas needing improvement so next versions build upon strengths\n",
            "while addressing weaknesses effectively   5) Final Prototype Refinement After\n",
            "several iterations refine components further ensuring alignment across\n",
            "aesthetics engineering performance standards set initially   Moving toward\n",
            "Production Readiness Once satisfied with TRL9 (Technology readiness level),\n",
            "ensure full documentation including Bills Of Materials BOMs detailed\n",
            "instructions packaging specifications‚Äîall critical steps needed prior scheduling\n",
            "assembly lines‚Ä¶  6) Tooling Development To Support MassProduction Create molds\n",
            "dies jigs fixtures tailored precisely according to finalized CAD drawings\n",
            "guarantee consistent output throughout manufacturing runs reducing waste via\n",
            "optimized tool setups‚Ä¶ This phase ensures efficiency scalability meeting\n",
            "quantity demands cost-effectively produced products maintaining quality control\n",
            "parameters established earlier stages  7 ) Quality Control Systems\n",
            "Implementation Define QC procedures inspect finished goods verifying adherence\n",
            "original specs identifying defects promptly rectifying them efficiently keeping\n",
            "end-users happy confident receiving items free faults improving overall brand\n",
            "reputation standing trustworthily market place long-term basis  Finally\n",
            "Transition From Proto shop Floor To Mainstream Manufacturing Partner With\n",
            "reliable manufacturers who've proven track record handling complex projects\n",
            "providing transparent communication channels regularly auditing productions\n",
            "minimizing risks discrepancies aligning deliverables consumer satisfaction\n",
            "metrics solidify business partnerships sustainable growth strategic planning\n",
            "supporting future innovations continuous improvements staying ahead competition\n",
            "continuously enhancing offerings fulfilling evolving client demand successfully\n",
            "converting conceptual ideas tangible realities achieving mutual success clients\n",
            "designers alike\n"
          ]
        }
      ]
    }
  ]
}