{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKQiT7xLP713IFPTaR2wnh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "894d6c038e9f4512927e5bef51c55c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970a56b310a342919407d2a097feb7fc",
              "IPY_MODEL_47c65a3bba794d89b59889998b49016d",
              "IPY_MODEL_165867cf9897409fad3e8bbd13d07adc"
            ],
            "layout": "IPY_MODEL_3b29e4e1eac34126b4e4a38901337353"
          }
        },
        "970a56b310a342919407d2a097feb7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467af45a1f9b4d73943f47bee1d3b804",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_965c8e1be0844f1e80e821d39d8578dc",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "47c65a3bba794d89b59889998b49016d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45890a737941405b82cb1e3edc3e9d2b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4186be55ff1b403495d20ad6babf919e",
            "value": 4
          }
        },
        "165867cf9897409fad3e8bbd13d07adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c7330f500ed4b9f8c6510383c294f84",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_21f332876ac040fa969815fedbb3792d",
            "value": "‚Äá4/4‚Äá[02:16&lt;00:00,‚Äá29.44s/it]"
          }
        },
        "3b29e4e1eac34126b4e4a38901337353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467af45a1f9b4d73943f47bee1d3b804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965c8e1be0844f1e80e821d39d8578dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45890a737941405b82cb1e3edc3e9d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4186be55ff1b403495d20ad6babf919e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c7330f500ed4b9f8c6510383c294f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21f332876ac040fa969815fedbb3792d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Getting Started with DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìå **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "---\n",
        "\n",
        "## **üöÄ Installing and running DeepSeek-R1-Distill-Llama-8B**  \n",
        "\n",
        "This notebook provides a **step-by-step guide** on how to **download and run DeepSeek-R1-Distill-Llama-8B** locally in **Google Drive**.  If you don't want to use Google Drive, you can install the artefacts on a local machine, server or cloud server.\n",
        "\n",
        "### **üîπ How to Get Started**  \n",
        "1Ô∏è‚É£ **Install the model's artifacts** ‚Üí Set `install_deepseek=True` and run all cells.  \n",
        "2Ô∏è‚É£ **Restart the session** ‚Üí Disconnect and start a new session.  \n",
        "3Ô∏è‚É£ **Re-run the model** ‚Üí Set `install_deepseek=False` and run all cells again.  \n",
        "4Ô∏è‚É£ **Interact with the model** ‚Üí Use it in a prompt session!  \n",
        "\n",
        "‚ö†Ô∏è **System Requirements**  \n",
        "‚úÖ **GPU** ‚Äì Minimum **16GB** VRAM required.  \n",
        "‚úÖ **Google Drive Space** ‚Äì At least **20GB** free space.  \n",
        "üìå **Educational Use Only** ‚Äì For production, deploy artifacts on a **local or cloud server**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìñ Table of Contents**  \n",
        "\n",
        "### **1Ô∏è‚É£ Setting Up the DeepSeek Environment (Hugging Face)**  \n",
        "‚úÖ Checking GPU Activation  \n",
        "üìÇ Mounting Google Drive  \n",
        "‚öôÔ∏è Installing the Hugging Face Environment  \n",
        "üîÑ Ensuring `install_deepseek=True` for First Run  \n",
        "üìå Checking Transformer Version  \n",
        "\n",
        "### **2Ô∏è‚É£ Downloading DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìÇ Verifying Download Path  \n",
        "\n",
        "### **3Ô∏è‚É£ Running a DeepSeek Session**  \n",
        "üîÑ Setting `install_deepseek=False` for Second Run  \n",
        "üìå Model Information  \n",
        "üí¨ Running an Interactive Prompt Session  \n",
        "\n",
        "---\n",
        "\n",
        "### **üí° Ready to Use DeepSeek?**  \n",
        "Follow the **installation steps**, ensure you have the required **hardware**, and launch your **interactive AI session** üöÄ"
      ],
      "metadata": {
        "id": "dB7uI-BJ94pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting up DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For installation, set to True\n",
        "# For running with pre-downloaded installation set to False\n",
        "install_deepseek=False"
      ],
      "metadata": {
        "id": "81qmq4cJ65_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lnpx8Ywvkqu",
        "outputId": "fbd6af7a-7297-4748-c400-5f1c19763c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb  7 14:58:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJAvtheKuudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e7577b-19c4-436a-eb41-4ac35e211bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the cache directory in your Google Drive\n",
        "cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "# Set environment variables to direct Hugging Face to use this cache directory\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "#os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07XwZO2Bx1sZ",
        "outputId": "382cfdb4-14e5-4bd3-8ac6-d8bb791cdce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"transformers\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking transformer version"
      ],
      "metadata": {
        "id": "yKXESfW5zzpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "-0uKNItkz_0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e103e32f-2be0-4b6f-f45f-b50c3e560708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.DeepSeek download"
      ],
      "metadata": {
        "id": "CMDClDdHJ1nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
      ],
      "metadata": {
        "id": "gO6E4YkUDNPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "if install_deepseek==True:\n",
        "   # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  model_name = 'unsloth/DeepSeek-R1-Distill-Llama-8B'\n",
        "  # Load the tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
        "\n",
        "    # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "FT1zO4ShzzON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verifying path, model and configuration"
      ],
      "metadata": {
        "id": "b2eFxJmhKlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        " !ls -R /content/drive/MyDrive/genaisys/HuggingFaceCache"
      ],
      "metadata": {
        "id": "zz0hLeEhJTt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        "  model_name"
      ],
      "metadata": {
        "id": "C7TJDLXzyes4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        "  model_name.config"
      ],
      "metadata": {
        "id": "ltwma4m-9sDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.DeepSeek-R1-Distill-Llama-8B session"
      ],
      "metadata": {
        "id": "mPFjy90U2gcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model information"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "if install_deepseek==False:\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "894d6c038e9f4512927e5bef51c55c79",
            "970a56b310a342919407d2a097feb7fc",
            "47c65a3bba794d89b59889998b49016d",
            "165867cf9897409fad3e8bbd13d07adc",
            "3b29e4e1eac34126b4e4a38901337353",
            "467af45a1f9b4d73943f47bee1d3b804",
            "965c8e1be0844f1e80e821d39d8578dc",
            "45890a737941405b82cb1e3edc3e9d2b",
            "4186be55ff1b403495d20ad6babf919e",
            "7c7330f500ed4b9f8c6510383c294f84",
            "21f332876ac040fa969815fedbb3792d"
          ]
        },
        "outputId": "11bb847e-036a-4330-b7eb-03894b8083f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "894d6c038e9f4512927e5bef51c55c79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 149.22 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnzQ7W7g8v15",
        "outputId": "8d13d04a-9dac-465a-814e-46a424f00feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSgiBU2m8rJJ",
        "outputId": "7bd492a7-fcec-4bbd-c5f2-8f800eff9e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"_attn_implementation_autoset\": true,\n",
              "  \"_name_or_path\": \"/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de\",\n",
              "  \"architectures\": [\n",
              "    \"LlamaForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 128000,\n",
              "  \"eos_token_id\": 128001,\n",
              "  \"head_dim\": 128,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 4096,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 14336,\n",
              "  \"max_position_embeddings\": 131072,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 32,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pad_token_id\": 128004,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-05,\n",
              "  \"rope_scaling\": {\n",
              "    \"factor\": 8.0,\n",
              "    \"high_freq_factor\": 4.0,\n",
              "    \"low_freq_factor\": 1.0,\n",
              "    \"original_max_position_embeddings\": 8192,\n",
              "    \"rope_type\": \"llama3\"\n",
              "  },\n",
              "  \"rope_theta\": 500000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.48.2\",\n",
              "  \"unsloth_fixed\": true,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 128256\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "QTSNYBCb4vtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  prompt=\"\"\"\n",
        "  Explain what the distillation process is and how it works. Explain how this applies to a DeepSeek-R1 model and how it differs from a DeepSeek-V3 model. Then explain\n",
        "  how to distill a DeepSeek-R1 model with Llama 8B.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "t99pgv0IQ30m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if install_deepseek==False:\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  # Tokenize the input\n",
        "  inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Generate output\n",
        "  outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "  # Decode and display the output\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "  #print(generated_text)"
      ],
      "metadata": {
        "id": "Qt9GTrB34r2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4126bb4-34d2-4ce6-a222-70da58cd471f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 2.06 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "if install_deepseek==False:\n",
        "  # Assuming 'generated_text' contains the text you want to format\n",
        "  wrapped_text = textwrap.fill(generated_text, width=80)  # Adjust 'width' as needed\n",
        "\n",
        "  print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "p4QX8niIK6Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b983e87b-6876-4f96-d833-7e944448cd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Explain what the distillation process is and how it works. Explain how this\n",
            "applies to a DeepSeek-R1 model and how it differs from a DeepSeek-V3 model. Then\n",
            "explain   how to distill a DeepSeek-R1 model with Llama 8B.       Okay, so I\n",
            "need to explain what distillation is and how it works. I remember hearing that\n",
            "it's a technique used in machine learning, maybe related to fine-tuning or\n",
            "transfer learning. Let me think. Distillation is like taking a\n"
          ]
        }
      ]
    }
  ]
}