{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNH/6En93sTLku/6qK8eFTt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df1a3d596d2e44dbb04cc9a9ce976ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74498fc31bc442878969b64d51942dda",
              "IPY_MODEL_5ad29cd82e4745d784b76db6cdfa6760",
              "IPY_MODEL_b2bedc6d02494f5da9b4e6595582cfa6"
            ],
            "layout": "IPY_MODEL_ede4825cd26045dd88f4e64d114bef80"
          }
        },
        "74498fc31bc442878969b64d51942dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3261852ffd294b6ca540901b95def77a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36de5e49bf384c538ee36b5626f96b88",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "5ad29cd82e4745d784b76db6cdfa6760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9157ce9268a4f2a9b2b0c264d9d3520",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_252dcac3089e4863b9a73248f4be09be",
            "value": 4
          }
        },
        "b2bedc6d02494f5da9b4e6595582cfa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d144ef791774a759158c5c61cc38474",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f8b8c8a55ac04f6a9c2ac8ab4f81a740",
            "value": "‚Äá4/4‚Äá[04:15&lt;00:00,‚Äá54.72s/it]"
          }
        },
        "ede4825cd26045dd88f4e64d114bef80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3261852ffd294b6ca540901b95def77a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36de5e49bf384c538ee36b5626f96b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9157ce9268a4f2a9b2b0c264d9d3520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252dcac3089e4863b9a73248f4be09be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d144ef791774a759158c5c61cc38474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b8c8a55ac04f6a9c2ac8ab4f81a740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Getting Started with DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìå **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "---\n",
        "\n",
        "## **üöÄ Installing and running DeepSeek-R1-Distill-Llama-8B**  \n",
        "\n",
        "This notebook provides a **step-by-step guide** on how to **download and run DeepSeek-R1-Distill-Llama-8B** locally in **Google Drive**.  The version downloaded is an open-source distilled version of DeepSeek-R1 provided by  unsloth, an LLM accelerator,  on Hugging Face :https://unsloth.ai/\n",
        "\n",
        "If you don't want to use Google Drive, you can install the artefacts on a local machine, server or cloud server.\n",
        "\n",
        "### **üîπ How to Get Started**  \n",
        "1Ô∏è‚É£ **Install the model's artifacts** ‚Üí Set `install_deepseek=True` and run all cells.  \n",
        "2Ô∏è‚É£ **Restart the session** ‚Üí Disconnect and start a new session.  \n",
        "3Ô∏è‚É£ **Re-run the model** ‚Üí Set `install_deepseek=False` and run all cells again.  \n",
        "4Ô∏è‚É£ **Interact with the model** ‚Üí Use it in a prompt session!  \n",
        "\n",
        "‚ö†Ô∏è **System Requirements**  \n",
        "‚úÖ **GPU** ‚Äì Minimum **16GB** VRAM required.  \n",
        "‚úÖ **Google Drive Space** ‚Äì At least **20GB** free space.  \n",
        "üìå **Educational Use Only** ‚Äì For production, deploy artifacts on a **local or cloud server**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìñ Table of Contents**  \n",
        "\n",
        "### **1Ô∏è‚É£ Setting Up the DeepSeek Environment (Hugging Face)**  \n",
        "‚úÖ Checking GPU Activation  \n",
        "üìÇ Mounting Google Drive  \n",
        "‚öôÔ∏è Installing the Hugging Face Environment  \n",
        "üîÑ Ensuring `install_deepseek=True` for First Run  \n",
        "üìå Checking Transformer Version  \n",
        "\n",
        "### **2Ô∏è‚É£ Downloading DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìÇ Verifying Download Path  \n",
        "\n",
        "### **3Ô∏è‚É£ Running a DeepSeek Session**  \n",
        "üîÑ Setting `install_deepseek=False` for Second Run  \n",
        "üìå Model Information  \n",
        "üí¨ Running an Interactive Prompt Session  \n",
        "\n",
        "---\n",
        "\n",
        "### **üí° Ready to Use DeepSeek?**  \n",
        "Follow the **installation steps**, ensure you have the required **hardware**, and launch your **interactive AI session** üöÄ"
      ],
      "metadata": {
        "id": "dB7uI-BJ94pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting up DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set install_deepseek to True to download and install R1 Distill Llama 8B locally\n",
        "# Set install_deepseek to False to run an R1 session\n",
        "install_deepseek=False"
      ],
      "metadata": {
        "id": "81qmq4cJ65_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lnpx8Ywvkqu",
        "outputId": "65405720-3fcf-4cbd-977f-a5314122014e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar  4 12:04:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJAvtheKuudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1050a4-40e5-46bf-9ee9-34039c9e4bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the cache directory in your Google Drive\n",
        "cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "# Set environment variables to direct Hugging Face to use this cache directory\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "#os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip transformers==4.48.3"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91686fba-cdf2-4f1e-d01d-4327c78d52aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"transformers==4.48.3\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.DeepSeek download\n",
        "\n"
      ],
      "metadata": {
        "id": "CMDClDdHJ1nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "if install_deepseek==True:\n",
        "   # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  model_name = 'unsloth/DeepSeek-R1-Distill-Llama-8B'\n",
        "  # Load the tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
        "\n",
        "    # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "FT1zO4ShzzON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        " !ls -R /content/drive/MyDrive/genaisys/HuggingFaceCache"
      ],
      "metadata": {
        "id": "zz0hLeEhJTt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.DeepSeek-R1-Distill-Llama-8B session"
      ],
      "metadata": {
        "id": "mPFjy90U2gcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "if install_deepseek==False:\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "df1a3d596d2e44dbb04cc9a9ce976ad9",
            "74498fc31bc442878969b64d51942dda",
            "5ad29cd82e4745d784b76db6cdfa6760",
            "b2bedc6d02494f5da9b4e6595582cfa6",
            "ede4825cd26045dd88f4e64d114bef80",
            "3261852ffd294b6ca540901b95def77a",
            "36de5e49bf384c538ee36b5626f96b88",
            "c9157ce9268a4f2a9b2b0c264d9d3520",
            "252dcac3089e4863b9a73248f4be09be",
            "2d144ef791774a759158c5c61cc38474",
            "f8b8c8a55ac04f6a9c2ac8ab4f81a740"
          ]
        },
        "outputId": "b5541219-c5af-4d8e-c599-35647f627ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df1a3d596d2e44dbb04cc9a9ce976ad9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 272.63 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  model.config"
      ],
      "metadata": {
        "id": "jSgiBU2m8rJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "QTSNYBCb4vtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  prompt=\"\"\"\n",
        "  Explain how a product designer could transformer customer requirements for a traveling bag into a production plan.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "t99pgv0IQ30m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if install_deepseek==False:\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  # Tokenize the input\n",
        "  inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Generate output with enhanced anti-repetition settings\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1200,\n",
        "    repetition_penalty=1.5,             # Increase penalty to 1.5 or higher\n",
        "    no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "    temperature=0.6,                    # Reduce randomness slightly\n",
        "    top_p=0.9,                          # Nucleus sampling for diversity\n",
        "    top_k=50                            # Limits token selection to top-k probable tokens\n",
        "  )\n",
        "\n",
        "  # Decode and display the output\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "  #print(generated_text)"
      ],
      "metadata": {
        "id": "Qt9GTrB34r2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d84c95-561a-4c38-e361-a24b852a7a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 32.03 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "if install_deepseek==False:\n",
        "  # Assuming 'generated_text' contains the text you want to format\n",
        "  wrapped_text = textwrap.fill(generated_text, width=80)  # Adjust 'width' as needed\n",
        "\n",
        "  print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "p4QX8niIK6Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39db495-cbae-47b0-8b56-3eb5e9f5052c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Explain how a product designer could transformer customer requirements for a\n",
            "traveling bag into a production plan.      Okay, so I need to figure out how\n",
            "someone with experience in designing products can take the needs of customers\n",
            "and turn that into something they can make. The specific example given is\n",
            "transforming \"customer requirement\" about a travel bag into what's needed on an\n",
            "assembly line.  Hmm... Let me break this down step by step because it seems like\n",
            "there are several stages involved here.  First off, understanding the problem\n",
            "thoroughly makes sense as a starting point‚Äîwhat exactly does the user want? They\n",
            "mentioned five main areas: Functionality, Aesthetic Design, Safety & Compliance,\n",
            "Cost Efficiency, Sustainability/Craftsmanship Level.  So maybe before anything\n",
            "else happens, you have meetings or surveys where potential users share their\n",
            "pain points regarding luggage bags. For instance, people might find current\n",
            "options too bulky when packed tightly but not organized enough otherwise. Or\n",
            "perhaps durability issues after some use‚Äîor even environmental concerns if\n",
            "sustainability comes up.  Once those insights from end-users (and existing data)\n",
            "come together through research methods like interviews or questionnaires‚Äîit‚Äôs\n",
            "important to prioritize which features matter most versus nice-to-haves. Like\n",
            "prioritizing ease-of-use over fancy exterior designs since functionality often\n",
            "takes precedence during travels compared to aesthetics once everyone has\n",
            "boarded.  Moving forward, creating detailed specifications sounds crucial next.\n",
            "You know your top three priorities now; translating these ideas into measurable\n",
            "parameters helps all parties stay aligned. Whether discussing materials‚Äîthe\n",
            "right combination between strength and weight‚Äîas well as dimensions precise\n",
            "enough without being overly restrictive‚Äîthat balance act must be deliberate\n",
            "based on feedback.  Design sketches would bring everything visualized\n",
            "properly‚Äîfor clarity, especially things hard to describe textually. Then\n",
            "prototyping becomes essential to test feasibility and gather real-world opinions\n",
            "again via testing sessions whether internally among team members or externally\n",
            "with select focus groups who‚Äôve been part of earlier input processes.  When\n",
            "thinking manufacturing process steps, material selection plays a big role‚Äîif\n",
            "we're aiming eco-friendly then using recycled fabrics will align both\n",
            "functionally sound yet sustainable choice-wise. Precision cutting patterns\n",
            "ensure efficiency while allowing room for variations due to handcrafted elements\n",
            "ensuring each piece fits just right regardless machine limitations sometimes\n",
            "present at scale levels beyond small batches initially?  Cost estimation\n",
            "alongside supply chain considerations should factor labor costs depending\n",
            "region, shipping logistics considering distances etc., plus any additional\n",
            "safety regulations applicable across different regions distributing the product\n",
            "globally requires compliance adaptability...  Quality control measures cannot go\n",
            "overlooked‚Äîthey set standards throughout every stage pre-production pilot runs\n",
            "help identify gaps refining procedures necessary to meet performance criteria\n",
            "consistently maintaining quality despite scaling operations post-pilot phase.)\n",
            "Scheduling timelines efficiently ensures smooth workflow preventing bottlenecks\n",
            "downstream impact teams upstream dealing fabric availability component lead\n",
            "times machinery maintenance planning ahead keeps projects moving smoothly rather\n",
            "than reacting delays causing chaos overall project timeline extension impacting\n",
            "deadlines further along pipeline affecting more resources unnecessarily!\n",
            "Communication channels centralize updates keeping stakeholders informed risks\n",
            "transparent build trust within cross-functional teams including management\n",
            "external partners feeling confident progress according planned schedule adhering\n",
            "budget constraints meeting client expectations effectively managing change\n",
            "requests handling unexpected challenges proactively minimizing disruption\n",
            "reaching final successful execution turning initial vague desires traveler\n",
            "solution robust practical durable high-quality reliable functional satisfying\n",
            "modern design integrating seamlessly everyday life making them feel good knowing\n",
            "invested thought gone into creation journey.   Wait did i miss anything? - Maybe\n",
            "also think ergonomics aspects‚Äîbut under 'functionality' already covered. Yeah\n",
            "probably covers key factors required transitioning customer reqs solidly\n",
            "grounded reality factory floor  I guess another thing is establishing clear\n",
            "milestones for development phases‚Äîfrom concept approval to prototype done early\n",
            "decision-making checkpoints prevents scope creep later costly adjustments\n",
            "avoiding missed specs critical details getting lost translation spec sheets\n",
            "proper documentation traceable version changes linked back original intent\n",
            "reducing misunderstandings suppliers interpretation same way comprehensive\n",
            "approach meticulous detail transforms ambiguous wants structured actionable\n",
            "plans ready massÁîü‰∫ß feasible efficient manner leveraging knowledge gained\n",
            "previous successes similar projects optimizing tried tested workflows saving\n",
            "time effort energy hitting mark perfectly fulfilling clients exact demands\n",
            "leading satisfied repeat business positive word mouth referral building brand\n",
            "loyalty reinforcing competitive edge industry\n"
          ]
        }
      ]
    }
  ]
}